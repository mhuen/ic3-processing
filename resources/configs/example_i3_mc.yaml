# Example config file to process Level 2 MC i3-files
#
# Directory structure:
#       At prompt of create_job_files.py a data_folder
#       will be asked for in which the files are to be
#       saved
#
#   Files are then stored as:
#
#   data_folder:
#       processing:
#           out_dir_pattern:
#               jobs: (job files are stored here)
#               logs: (log files are stored here)
#
#       out_dir_pattern:
#
#               folder_pattern:
#                       out_file_pattern + '.' + i3_ending
#                       out_file_pattern + '.hdf5'
#
#
#               Where the run folder is given by:
#               folder_pattern = folder_pattern.format(
#                   folder_num=folder_num,
#                   folder_num_pre_offset=folder_num_pre_offset,
#                   folder_num_n_merged=folder_num_n_merged,
#                   folder_num_pre_offset_n_merged=folder_num_pre_offset_n_merged,
#                   **cfg
#               )
#
#       The following variables are computed and can be used in input/output patterns
#
#       folder_num = folder_offset + run_number // n_jobs_per_folder
#       folder_num_pre_offset = run_number // n_jobs_per_folder
#       folder_num_n_merged = folder_offset + (n_runs_per_merge * run_number) // n_jobs_per_folder
#       folder_num_pre_offset_n_merged = (n_runs_per_merge * run_number) // n_jobs_per_folder
#

#------------------------------
# General job submission config
#------------------------------

keep_crashed_files: False

resources:
        # If gpus == 1 this will be run on a GPU with
        gpus: 0
        cpus: 1
        memory: 2gb
        # has_ssse3: True
        # has_avx2: True

dagman_max_jobs: 5000
dagman_submits_interval: 500
dagman_scan_interval: 1
dagman_submit_delay: 0

# If true, the input files will first be checked for corruption.
# Note that this will take a while, since each input file has to be
# iterated through. You generally only want to set this to true if you
# are merging a number of input files of which some are known to be corrupt.
exclude_corrupted_input_files: False

#------------------------------
# Define Datasets to process
#------------------------------

#------
# common settings shared by all datasets
#------
i3_ending: 'i3.zst'
in_file_pattern: '/data/sim/IceCube/2020/filtered/level2/neutrino-generator/{dataset_number}/{folder_pattern}/Level2_{year}_{flavor}.{dataset_number:06d}.{run_number:06d}.i3.zst'
out_dir_pattern: '{data_type}/{dataset_number}/'
out_file_pattern: '{data_type}_{dataset_number}_{run_number:08d}'
folder_pattern: '{folder_num_pre_offset:04d}000-{folder_num_pre_offset:04d}999'
folder_offset: 0
n_jobs_per_folder: 1000
gcd: /cvmfs/icecube.opensciencegrid.org/data/GCD/GeoCalibDetectorStatus_2020.Run134142.Pass2_V0.i3.gz
#------


datasets:

    # ------------
    # Spice BFR-v1
    # ------------
    NuGen_NuMu_bfrv1:
        cycler:
                dataset_number: [21535]

        runs_range: [0, 2] # 2000
        data_type: 'NuGen'
        flavor: 'NuMu'
        year: 'IC86.2020'

    NuGen_NuTau_bfrv1:
        cycler:
                dataset_number: [21536]

        runs_range: [0, 2] # 2000
        data_type: 'NuGen'
        flavor: 'NuTau'
        year: 'IC86.2020'

    NuGen_NuE_bfrv1:
        cycler:
                dataset_number: [21537]

        runs_range: [0, 2] # 2000
        data_type: 'NuGen'
        flavor: 'NuE'
        year: 'IC86.2020'


# -------------------------------------------------------------
# Define environment information shared across processing steps
# -------------------------------------------------------------
job_template: job_templates/cvmfs_python.sh
script_name: general_i3_processing.py
python_user_base_cpu: /data/user/mhuennefeld/DNN_reco/virtualenvs/tensorflow_gpu_py3-v4.2.1
python_user_base_gpu: /data/user/mhuennefeld/DNN_reco/virtualenvs/tensorflow_gpu_py3-v4.2.1
cuda_home: /data/ana/PointSource/DNNCascade/utils/cuda/cuda-10.1

# add optional additions to the LD_LIBRARY_PATH
# Note: '{ld_library_path_prepends}' is the default which does not add anything
ld_library_path_prepends: '{ld_library_path_prepends}'

# add optional python packages to import as a comma separated list
# Note: '{python_package_imports}' is the default which does not add anything
python_package_imports: '{python_package_imports}'

# Defines environment variables that are set from python
set_env_vars_from_python: {
    'TF_DETERMINISTIC_OPS': '1',
}

#-----------------------------------------------
# Define I3Traysegments for each processing step
#-----------------------------------------------

# a list of processing steps. Each processing step contains
# information on the python and cvmfs environment as well as
# a list of I3TraySegments/Modules that will be added to the I3Tray.
# Any options defined in these nested dictionaries will supersede the
# ones defined globally in this config.
# Tray context can be accessed via "context-->key".
# For nested dictionaries it's possible to do: "context-->key.key2.key3"
# The configuration dictionary of the job can be passed via "<config>"
# Special keys for the tray_segments:
#       ModuleClass: str
#           The module/segment to run.
#       ModuleKwargs: dict
#           The parameters for the specified module.
#       ModuleTimer: str
#           If provided, a timer for this module will be added.
#           Results of all timers are saved in the frame key "Duration".
processing_steps: [

    # -----------------
    # Processing step 1
    # -----------------
    {
        # Define environment for this processing step
        cvmfs_python: py3-v4.2.1,
        icetray_metaproject: icetray/v1.5.1,

        # define a list of tray segments to run
        tray_segments: [
            {
                # compute weights
                ModuleClass: 'ic3_labels.weights.segments.WeightEvents',
                ModuleKwargs: {
                    'infiles': context-->ic3_processing.infiles,
                    'dataset_type': '{data_type}',
                    'dataset_n_files': context-->ic3_processing.n_files,
                    'dataset_n_events_per_run': -1,
                    'dataset_number': '{dataset_number}',
                    'check_n_files': ['corsika', 'nugen'],
                    'add_mceq_weights': False,
                    'mceq_kwargs': {
                        'cache_file': '/data/ana/PointSource/DNNCascade/utils/cache/mceq.cache',
                    },
                },
            },
        ],
    },

    # -----------------
    # Processing step 2
    # -----------------
    {
        # Define environment for this processing step
        cvmfs_python: py3-v4.2.1,
        icetray_metaproject: icetray/v1.5.1,

        # define a list of tray segments to run
        tray_segments: [
            {
                # delete the key "I3MCTree" from the i3 file
                ModuleClass: 'Delete',
                ModuleKwargs: {Keys: [I3MCPESeriesMapWithoutNoise]},
            },
        ],
    },
]

#--------------------
# File output options
#--------------------

# write output as i3 files via the I3Writer
write_i3: True
write_i3_kwargs: {

    # only write these stream types,
    # i.e ['Q', 'P', 'I', 'S', 'M', 'm', 'W', 'X']
    'i3_streams': ['Q', 'P', 'I', 'S', 'M', 'm', 'W', 'X'],
}

# write output as hdf5 files via the I3HDFWriter
write_hdf5: False
write_hdf5_kwargs: {

    # sub event streams to write
    'SubEventStreams': ['InIceSplit'],

    # HDF keys to write (in addition to the ones in
    # tray.context['ic3_processing']['HDF_keys'])
    # Note: added tray segments should add outputs that should be written
    # to hdf5 to the tray context.
    'Keys': [],
}
